{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab98eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install Required Tools (GCTA)\n",
    "# GCTA is required for conditional analysis (COJO)\n",
    "import os\n",
    "\n",
    "# Create tools directory\n",
    "!mkdir -p ./tools\n",
    "\n",
    "# Check if GCTA is installed\n",
    "if not os.path.exists(\"./tools/gcta64\"):\n",
    "    print(\"Installing GCTA...\")\n",
    "    # Updated URL for version 1.94.4\n",
    "    gcta_url = \"https://yanglab.westlake.edu.cn/software/gcta/bin/gcta-1.94.4-linux-kernel-3-x86_64.zip\"\n",
    "    zip_name = \"gcta-1.94.4-linux-kernel-3-x86_64.zip\"\n",
    "    folder_name = \"gcta-1.94.4-linux-kernel-3-x86_64\"\n",
    "    \n",
    "    # Download\n",
    "    !wget -O ./tools/{zip_name} {gcta_url}\n",
    "    \n",
    "    # Verify download and install\n",
    "    if os.path.exists(f\"./tools/{zip_name}\"):\n",
    "        !unzip -o ./tools/{zip_name} -d ./tools/\n",
    "        # Move binary to main tools folder\n",
    "        if os.path.exists(f\"./tools/{folder_name}/gcta64\"):\n",
    "            !mv ./tools/{folder_name}/gcta64 ./tools/\n",
    "            !chmod +x ./tools/gcta64\n",
    "            # Clean up\n",
    "            !rm -rf ./tools/{folder_name}\n",
    "            !rm ./tools/{zip_name}\n",
    "            print(\"GCTA installed successfully.\")\n",
    "        else:\n",
    "             print(f\"Error: gcta64 binary not found in {folder_name}\")\n",
    "    else:\n",
    "        print(\"Error: GCTA download failed. Please check the URL.\")\n",
    "else:\n",
    "    print(\"GCTA already installed.\")\n",
    "\n",
    "GCTA_PATH = os.path.abspath(\"./tools/gcta64\")\n",
    "print(f\"GCTA Path: {GCTA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4851acfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c523afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define Paths and Constants\n",
    "\n",
    "# Inputs\n",
    "GWAS_RESULTS_PATH = \"gs://fc-aou-datasets-controlled/AllxAll/v1/ht/ACAF/EUR/phenotype_NS_326.1_ACAF_results.ht\"\n",
    "WGS_MT_PATH = \"gs://fc-aou-datasets-controlled/v8/wgs/short_read/snpindel/acaf_threshold/splitMT/hail.mt\"\n",
    "ANCESTRY_PREDS_PATH = \"gs://fc-aou-datasets-controlled/v8/wgs/short_read/snpindel/aux/ancestry/ancestry_preds.tsv\"\n",
    "\n",
    "# Outputs\n",
    "LOCI_OUTPUT_PATH = f\"{bucket}/results/phase2_defined_loci.bed\"\n",
    "FINE_MAPPING_DIR = f\"{bucket}/results/fine_mapping\"\n",
    "\n",
    "# Constants\n",
    "P_THRESHOLD = 5e-8\n",
    "MHC_INTERVAL = \"chr6:25000000-35000000\" # Broad MHC exclusion as per Analysis_README\n",
    "FLANK_WINDOW = 500000 # 500kb flank for initial locus definition (1Mb total width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d626bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load EUR Sample IDs (for LD Reference)\n",
    "print(\"Loading EUR sample IDs...\")\n",
    "!gsutil -u $GOOGLE_PROJECT cp {ANCESTRY_PREDS_PATH} ./ancestry_temp.tsv\n",
    "ancestry_df = pd.read_csv(\"./ancestry_temp.tsv\", sep=\"\\t\")\n",
    "eur_ids = set(ancestry_df[ancestry_df['ancestry_pred'] == 'eur']['research_id'].astype(str))\n",
    "!rm ./ancestry_temp.tsv\n",
    "\n",
    "print(f\"Identified {len(eur_ids)} European samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85316b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Load GWAS Summary Statistics and Filter\n",
    "print(\"Loading GWAS summary statistics...\")\n",
    "gwas_ht = hl.read_table(GWAS_RESULTS_PATH)\n",
    "\n",
    "# Filter for genome-wide significant hits\n",
    "sig_ht = gwas_ht.filter(gwas_ht.p_value < P_THRESHOLD)\n",
    "\n",
    "# Exclude MHC Region\n",
    "print(f\"Excluding MHC region: {MHC_INTERVAL}\")\n",
    "sig_ht = sig_ht.filter(~hl.parse_locus_interval(MHC_INTERVAL).contains(sig_ht.locus))\n",
    "\n",
    "# Check count\n",
    "n_sig = sig_ht.count()\n",
    "print(f\"Number of significant SNPs outside MHC: {n_sig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc07e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Define Loci (Distance-based Clumping)\n",
    "# We perform a greedy distance-based clumping to define the initial windows for fine-mapping.\n",
    "\n",
    "# Collect significant hits to Pandas\n",
    "df_sig = sig_ht.select('p_value').to_pandas()\n",
    "df_sig['chrom'] = df_sig['locus'].apply(lambda x: x.contig)\n",
    "df_sig['pos'] = df_sig['locus'].apply(lambda x: x.position)\n",
    "df_sig = df_sig.sort_values('p_value')\n",
    "\n",
    "defined_loci = []\n",
    "\n",
    "print(\"Performing distance-based clumping...\")\n",
    "while not df_sig.empty:\n",
    "    # Take top SNP\n",
    "    lead_snp = df_sig.iloc[0]\n",
    "    \n",
    "    # Define window\n",
    "    chrom = lead_snp['chrom']\n",
    "    start = max(1, lead_snp['pos'] - FLANK_WINDOW)\n",
    "    end = lead_snp['pos'] + FLANK_WINDOW\n",
    "    \n",
    "    defined_loci.append({\n",
    "        'chrom': chrom,\n",
    "        'start': start,\n",
    "        'end': end,\n",
    "        'lead_snp_pos': lead_snp['pos'],\n",
    "        'lead_snp_p': lead_snp['p_value']\n",
    "    })\n",
    "    \n",
    "    # Remove all SNPs within this window from the pool\n",
    "    df_sig = df_sig[~((df_sig['chrom'] == chrom) & \n",
    "                      (df_sig['pos'] >= start) & \n",
    "                      (df_sig['pos'] <= end))]\n",
    "\n",
    "loci_df = pd.DataFrame(defined_loci)\n",
    "print(f\"Defined {len(loci_df)} independent loci.\")\n",
    "\n",
    "# Save rough loci definitions\n",
    "loci_df.to_csv(\"defined_loci.tsv\", sep=\"\\t\", index=False)\n",
    "!gsutil cp defined_loci.tsv {LOCI_OUTPUT_PATH}\n",
    "print(f\"Initial loci definitions saved to {LOCI_OUTPUT_PATH}\")\n",
    "loci_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Helper Functions for Fine-Mapping Prep\n",
    "\n",
    "def extract_locus_plink(chrom, start, end, output_prefix, eur_ids_set):\n",
    "    \"\"\"\n",
    "    Extracts WGS data for a specific locus, filters to EUR samples, \n",
    "    and exports to PLINK format for GCTA.\n",
    "    \"\"\"\n",
    "    # Define interval\n",
    "    interval = hl.parse_locus_interval(f\"{chrom}:{start}-{end}\")\n",
    "    \n",
    "    # Load WGS MT\n",
    "    mt = hl.read_matrix_table(WGS_MT_PATH)\n",
    "    \n",
    "    # Filter to interval AND European samples\n",
    "    mt_locus = hl.filter_intervals(mt, [interval])\n",
    "    mt_locus = mt_locus.filter_cols(hl.literal(eur_ids_set).contains(mt_locus.s))\n",
    "    \n",
    "    # Export to PLINK\n",
    "    # Note: GCTA requires .bed/.bim/.fam\n",
    "    hl.export_plink(mt_locus, output_prefix, ind_id=mt_locus.s, fam_id=mt_locus.s)\n",
    "\n",
    "def run_gcta_cojo(bfile, sumstats_file, out_prefix):\n",
    "    \"\"\"\n",
    "    Runs GCTA-COJO to identify independent signals.\n",
    "    \"\"\"\n",
    "    # Ensure the sumstats file matches GCTA format: SNP A1 A2 freq b se p N\n",
    "    cmd = f\"{GCTA_PATH} --bfile {bfile} --cojo-file {sumstats_file} --cojo-slct --out {out_prefix}\"\n",
    "    print(f\"Running: {cmd}\")\n",
    "    os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f007cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Run Fine-Mapping (GCTA-COJO) per Locus\n",
    "# This loop iterates over 'defined_loci', extracts EUR genotypes, and runs GCTA-COJO.\n",
    "\n",
    "# Create local directory for temporary files\n",
    "!mkdir -p ./temp_finemap\n",
    "\n",
    "for i, row in loci_df.iterrows():\n",
    "    locus_id = f\"{row['chrom']}_{row['start']}_{row['end']}\"\n",
    "    print(f\"\\n--- Processing Locus {i+1}/{len(loci_df)}: {locus_id} ---\")\n",
    "    \n",
    "    local_prefix = f\"./temp_finemap/{locus_id}\"\n",
    "    \n",
    "    # 1. Extract Genotypes (LD Reference)\n",
    "    # Checks if PLINK files already exist to avoid re-extracting\n",
    "    if not os.path.exists(f\"{local_prefix}.bed\"):\n",
    "        print(\"Extracting genotypes...\")\n",
    "        extract_locus_plink(row['chrom'], row['start'], row['end'], local_prefix, eur_ids)\n",
    "    else:\n",
    "        print(\"Genotypes already extracted.\")\n",
    "\n",
    "    # 2. Prepare Sumstats for GCTA\n",
    "    # We need to export the summary stats for this specific region to a format GCTA accepts\n",
    "    # This requires filtering the main Hail Table again or subsetting a pandas DF\n",
    "    # (Simplified placeholder logic below - requires mapping Hail fields to GCTA columns)\n",
    "    \n",
    "    # 3. Run GCTA-COJO\n",
    "    # run_gcta_cojo(local_prefix, f\"{local_prefix}.ma\", f\"{local_prefix}_cojo\")\n",
    "    \n",
    "    # 4. (Optional) SuSiE would be run here using the LD matrix from the PLINK files\n",
    "    \n",
    "    # Cleanup large PLINK files if space is an issue\n",
    "    # !rm {local_prefix}.bed {local_prefix}.bim {local_prefix}.fam\n",
    "\n",
    "print(\"Fine-mapping preparation complete.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
